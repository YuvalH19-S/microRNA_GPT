{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad909992-911d-4c58-9280-637b4eddce60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cuda-toolkit-11.3 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cuda-toolkit-11.3\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q gpt_utils\n",
    "!pip install -q pandas\n",
    "!pip install -q cuda-toolkit-11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410aa5ae-1dad-43d6-aae0-f1dde0246c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import *\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from gpt_utils import *\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from ipywidgets import *\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import RNA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import Trainer, GPT2LMHeadModel, AutoTokenizer\n",
    "import transformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d119d742-7032-40f9-806e-a969bbeaeaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_dot_bracket(structure, sequence):\n",
    "    stack = []\n",
    "    valid_pairs = [('T', 'G'), ('G', 'T'), ('C', 'G'), ('G', 'C'), ('A', 'T'), ('T', 'A')]\n",
    "    invalid_pairs_dict = {}\n",
    "    \n",
    "    for i, char in enumerate(structure):\n",
    "        if char == '(':\n",
    "            stack.append((i, sequence[i]))\n",
    "        elif char == ')':\n",
    "            if not stack:\n",
    "                return False, invalid_pairs_dict\n",
    "            else:\n",
    "                position, nucleotide = stack.pop()\n",
    "                if (nucleotide, sequence[i]) not in valid_pairs:\n",
    "                    invalid_pair = f\"{nucleotide}{sequence[i]}\"\n",
    "                    invalid_pairs_dict[invalid_pair] = invalid_pairs_dict.get(invalid_pair, 0) + 1\n",
    "                    \n",
    "    # Check if there are any unmatched parentheses left\n",
    "    if stack:\n",
    "        return False, invalid_pairs_dict\n",
    "    \n",
    "    # Check if any invalid pairs were found\n",
    "    if invalid_pairs_dict:\n",
    "        return False, invalid_pairs_dict\n",
    "        \n",
    "    return True, invalid_pairs_dict\n",
    "\n",
    "def find_closing_bracket(struct_list):\n",
    "    # This function finds the index of the matching closing bracket.\n",
    "    depth = 1\n",
    "    for i, char in enumerate(struct_list):\n",
    "        if char == '(':\n",
    "            depth += 1\n",
    "        elif char == ')':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return i\n",
    "    raise ValueError(\"No matching closing bracket found\")\n",
    "    \n",
    "def mature_or_star(row):\n",
    "    full_seq = row['full_seq']\n",
    "    mature_start = full_seq.find(row['Mature'])\n",
    "    star_start = full_seq.find(row['Star'])\n",
    "    star_end = star_start + len(row['Star'])\n",
    "    \n",
    "    encoded_seq = full_seq\n",
    "    if star_start < star_end:\n",
    "        if mature_start < star_start:\n",
    "            mature_sequences.append(encoded_seq)\n",
    "        else:\n",
    "            star_sequences.append(encoded_seq)\n",
    "\n",
    "def encode_rna(row):\n",
    "    full_seq = row[relevant_seq]\n",
    "    mature_start = full_seq.find(row['Mature'])\n",
    "    if mature_start == -1:\n",
    "        print(\"no mature\")\n",
    "    mature_end = mature_start + len(row['Mature'])\n",
    "    star_start = full_seq.find(row['Star'])\n",
    "    if star_start == -1:\n",
    "        print(\"no star\")\n",
    "    star_end = star_start + len(row['Star'])\n",
    "    # star_end = len(row['Star'])\n",
    "\n",
    "    encoded_seq = full_seq\n",
    "    if star_start < star_end:\n",
    "        if mature_start < star_start:\n",
    "            encoded_seq = (encoded_seq[:mature_start] + 'ZZZZZ' +\n",
    "                           encoded_seq[mature_start:mature_end] + 'BBBBB' +\n",
    "                           encoded_seq[mature_end:star_start] + 'DDDDD' +\n",
    "                           encoded_seq[star_start:star_end] + 'FFFFF' +\n",
    "                           encoded_seq[star_end:])\n",
    "        else:\n",
    "            encoded_seq = (encoded_seq[:star_start] + 'DDDDD' +\n",
    "                           encoded_seq[star_start:star_end] + 'FFFFF' +\n",
    "                           encoded_seq[star_end:mature_start] + 'ZZZZZ' +\n",
    "                           encoded_seq[mature_start:mature_end] + 'BBBBB' +\n",
    "                           encoded_seq[mature_end:])\n",
    "    return encoded_seq\n",
    "\n",
    "def decode_rna(encoded_seq):\n",
    "    decoded_seq=  ['']*1000\n",
    "    # decoded_struct = ['']*1000\n",
    "    pair_dict_reverse = {'V': 'GC', 'W': 'CG','X': 'AT', 'Y': 'TA','N': 'GT', 'M': 'TG'}\n",
    "    \n",
    "    curr_index = len(encoded_seq) - 1\n",
    "    i=0\n",
    "    while curr_index >= 0:\n",
    "        char = encoded_seq[curr_index]\n",
    "        if char.isdigit():\n",
    "            # distance can be 2 or 3 consecutive digits from curr_index\n",
    "            if encoded_seq[curr_index-2].isdigit():\n",
    "                distance = encoded_seq[curr_index-2:curr_index+1]\n",
    "                curr_index -= 3\n",
    "            else:\n",
    "                distance = encoded_seq[curr_index-1:curr_index+1]\n",
    "                curr_index -= 2\n",
    "            pair =  pair_dict_reverse.get(encoded_seq[curr_index])\n",
    "            base1,base2 = pair[0],pair[1]\n",
    "            decoded_seq.insert(i,base1) #; decoded_struct.insert(i,'(')\n",
    "            # print(decoded_seq)\n",
    "            decoded_seq.insert(i-int(distance),base2) #; decoded_struct.insert(i-int(distance),')')\n",
    "            i+=2\n",
    "\n",
    "        else:\n",
    "            decoded_seq.insert(i,encoded_seq[curr_index])\n",
    "            i+=1\n",
    "        curr_index-=1\n",
    "\n",
    "    decoded_seq = ''.join(decoded_seq)[::-1]\n",
    "\n",
    "    return decoded_seq#, decoded_struct\n",
    "\n",
    "def extra_decode_rna(encoded_seq):\n",
    "    # Decoding logic you provided\n",
    "    decoded_seq = ['']*1000\n",
    "    decoded_struct = ['']*1000\n",
    "    pair_dict_reverse = {'V': 'GC', 'W': 'CG','X': 'AT', 'Y': 'TA','N': 'GT', 'M': 'TG'}\n",
    "\n",
    "    curr_index = len(encoded_seq) - 1\n",
    "    i=0\n",
    "    while curr_index >= 0:\n",
    "        char = encoded_seq[curr_index]\n",
    "        if char.isdigit():\n",
    "            # distance can be 2 or 3 consecutive digits from curr_index\n",
    "            if encoded_seq[curr_index-2].isdigit():\n",
    "                distance = encoded_seq[curr_index-2:curr_index+1]\n",
    "                curr_index -= 3\n",
    "            else:\n",
    "                distance = encoded_seq[curr_index-1:curr_index+1]\n",
    "                curr_index -= 2\n",
    "            pair =  pair_dict_reverse.get(encoded_seq[curr_index])\n",
    "            base1,base2 = pair[0],pair[1]\n",
    "            decoded_seq.insert(i,base1) ; decoded_struct.insert(i,'(')\n",
    "            decoded_seq.insert(i-int(distance),base2) ; decoded_struct.insert(i-int(distance),')')\n",
    "            i+=2\n",
    "\n",
    "        else:\n",
    "            decoded_seq.insert(i,encoded_seq[curr_index])\n",
    "            # print(i,encoded_seq[curr_index])\n",
    "            decoded_struct.insert(i,'.')\n",
    "            i+=1\n",
    "        curr_index-=1\n",
    "\n",
    "    decoded_seq = ''.join(decoded_seq)[::-1]\n",
    "    decoded_struct = ''.join(decoded_struct)[::-1]\n",
    "\n",
    "    # Replace T with U in the RNA sequence\n",
    "    decoded_seq = decoded_seq.replace('T', 'U')\n",
    "    \n",
    "    # Extract the mature and star sections\n",
    "    mature_start = decoded_seq.find('ZZZZZ') + 5\n",
    "    mature_end = decoded_seq.find('BBBBB')\n",
    "    star_start = decoded_seq.find('DDDDD') + 5\n",
    "    star_end = decoded_seq.find('FFFFF')\n",
    "    \n",
    "    # Extract the mature and star sections\n",
    "    mature_section = decoded_seq[mature_start:mature_end]\n",
    "    star_section = decoded_seq[star_start:star_end]\n",
    "\n",
    "    # Remove the markers and construct the full sequence structure\n",
    "    if mature_start > star_start:\n",
    "        # If mature section appears before the star section\n",
    "        full_seq_struct = (decoded_struct[:star_start - 5] +\n",
    "                           decoded_struct[star_start :star_end] + \n",
    "                           decoded_struct[star_end+5 :mature_start - 5] + \n",
    "                           decoded_struct[mature_start:mature_end]+ \n",
    "                           decoded_struct[mature_end+5:])\n",
    "    else:\n",
    "        # If mature section appears before the star section\n",
    "        full_seq_struct = (decoded_struct[:mature_start - 5] +\n",
    "                           decoded_struct[mature_start :mature_end] + \n",
    "                           decoded_struct[mature_end+5 :star_start - 5] + \n",
    "                           decoded_struct[star_start:star_end]+ \n",
    "                           decoded_struct[star_end+5:])\n",
    "\n",
    "    # Remove the markers and replace 'T' with 'U' in the full sequence\n",
    "    full_seq = decoded_seq.replace('ZZZZZ', '').replace('BBBBB', '').replace('DDDDD', '').replace('FFFFF', '')\n",
    "\n",
    "    return full_seq, full_seq_struct, mature_section, star_section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb89a5f3-bc9a-45cd-a186-57255984a66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fasta_to_tuples(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read().strip().split(\"\\n\")\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(0, len(content), 2):\n",
    "        header = content[i][1:]  # remove '>'\n",
    "        sequence = content[i+1]\n",
    "        sequences.append((header, sequence))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def tuples_to_jsonl(sequences, output_filename):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for header, sequence in sequences:\n",
    "            data = {\n",
    "                \"id\": header,\n",
    "                \"sequence\": sequence\n",
    "            }\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def calculate_statistics(sequences):\n",
    "    num_sequences = len(sequences)\n",
    "    avg_length = int(sum(len(seq) for seq in sequences) / num_sequences)\n",
    "    min_length = min(len(seq) for seq in sequences)\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    return {\n",
    "        \"Number of sequences\": num_sequences,\n",
    "        \"Average sequence length\": avg_length,\n",
    "        \"Min sequence length\": min_length,\n",
    "        \"Max sequence length\": max_length\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601206d4-ef12-42dd-92ac-344c7b8f002b",
   "metadata": {},
   "source": [
    "### Load Human Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820993b6-c274-4381-8237-d44ac84f947a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0041d353-2543-408b-b4e5-e2de2b44112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len after is in:  701 567\n",
      "len after remove duplicates 539\n",
      "len after removing flanks:  530\n"
     ]
    }
   ],
   "source": [
    "all_data = fasta_to_tuples(\"/sise/vaksler-group/IsanaRNA/Transformers/GPT_env/Data_source/miRGeneDB/precursors_all_species_w_flank.fa\")\n",
    "original_human_sequences = [s[1] for s in all_data  if s[0].startswith('Hsa')]\n",
    "\n",
    "preprocess_data = pd.read_csv(\"/sise/vaksler-group/IsanaRNA/Transformers/GPT_env/Data_source/Data_output/miRGeneDB_output/miRGeneDB_features.csv\")\n",
    "\n",
    "human_data = preprocess_data[preprocess_data['full_seq'].isin(original_human_sequences)]\n",
    "print(\"len after is in: \", len(human_data), len(original_human_sequences)) # duplicates (?)\n",
    "\n",
    "human_data = human_data.drop_duplicates(subset=['full_seq'], keep='first')\n",
    "print(\"len after remove duplicates\", len(human_data)) # after drop duplicates\n",
    "\n",
    "\n",
    "# ** without flanks: ** - if you want with flank, make the next lines in comment\n",
    "human_data_fasta = fasta_to_tuples(\"/sise/vaksler-group/IsanaRNA/Transformers/GPT_env/Data_source/miRGeneDB/precursors_human_no_flank.fas.txt\")\n",
    "original_human_sequences = [s[1] for s in human_data_fasta]\n",
    "\n",
    "## remove flanks from sequences\n",
    "human_data = human_data.dropna(subset=['full_seq', 'flank1', 'flank2']); # found some null flanks\n",
    "for index, row in human_data.iterrows():\n",
    "    human_data.at[index, 'full_seq'] = human_data.at[index, 'full_seq'].replace(row['flank1'], '').replace(row['flank2'], '')\n",
    "print(\"len after removing flanks: \", len(human_data)) # after drop duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d377b60c-ccd6-4e3f-ac1e-3a5f12999208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_seq = 'full_seq'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d1446-296c-4e79-bf93-d51b1dcf0c0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f4ecc1-8344-4ba0-bf5f-df91fe2b0863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# True if no flanks\n",
    "hairpin_bool = False\n",
    "if hairpin_bool:\n",
    "    relevant_seq = 'pre_mirna'\n",
    "else:\n",
    "    relevant_seq = 'full_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa56be7-c84a-41cc-9d0e-f890fa5212fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_data = pd.read_csv(\"/sise/vaksler-group/IsanaRNA/Transformers/GPT_env/seq_clusters/human_train_data.csv\")\n",
    "original_human_sequences = human_data[relevant_seq].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92ac76d-43cb-4656-9e23-506c1243decc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows with invalid dot-bracket language\n",
      "True    389\n",
      "Name: count, dtype: int64\n",
      "389 total of 0 removed\n"
     ]
    }
   ],
   "source": [
    "human_data = human_data.dropna(subset=['Star','Mature']) # found some null star and mature\n",
    "\n",
    "human_data.loc[:, 'encoded_seq'] = human_data.apply(encode_rna, axis=1)\n",
    "len_before = len(human_data)\n",
    "human_data = human_data[human_data['encoded_seq'] != -1]\n",
    "print(f\"Removed {len_before - len(human_data)} rows with invalid dot-bracket language\")\n",
    "\n",
    "human_data['decoded_seq'] =  human_data['encoded_seq'].apply(decode_rna) #zip(*human_data['encoded_seq'].progress_apply(decode_rna)) #,human_data['decoded_struct']\n",
    "human_data['decoded_seq'] =  human_data['decoded_seq'].apply(lambda x: x.replace('DDDDD','').replace('FFFFF','').replace('ZZZZZ','').replace('BBBBB',''))\n",
    "\n",
    "print(np.equal(human_data['decoded_seq'], human_data[relevant_seq]).value_counts())\n",
    "\n",
    "\n",
    "index_to_remove = []\n",
    "for i in range(len(human_data)):\n",
    "    if list(human_data['decoded_seq'])[i] != list(human_data[relevant_seq])[i]:\n",
    "        index_to_remove.append(i)\n",
    "human_data.drop(human_data.index[index_to_remove], inplace=True)\n",
    "print(len(human_data),f'total of {len(index_to_remove)} removed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c303a69-6c22-455d-951d-f69140033e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_human_sequences = ['<SOS>' + sequence + '<EOS>' for sequence in human_data['encoded_seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28aea57d-8084-42f3-a2f5-2cf61277af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of sequences': 389,\n",
       " 'Average sequence length': 149,\n",
       " 'Min sequence length': 142,\n",
       " 'Max sequence length': 156}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_statistics(encoded_human_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086e799-7a79-4e79-8ff6-ed4d33295c65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train New BPE tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf61cb-0dba-43d5-820e-96fa8a7e921f",
   "metadata": {},
   "source": [
    "**no need to train, use the tokenizer from the beggining of the training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bea81f9-f61c-4223-9199-fa6941605b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_statistics(tokenizer, sequences):\n",
    "    token_lengths = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        tokens = tokenizer.encode(seq).tokens\n",
    "        token_lengths.extend([len(token) for token in tokens])\n",
    "\n",
    "    avg_length = np.mean(token_lengths)\n",
    "    min_length = np.min(token_lengths)\n",
    "    max_length = np.max(token_lengths)\n",
    "    median_length = np.median(token_lengths)\n",
    "    std_dev = np.std(token_lengths)\n",
    "\n",
    "    return {\n",
    "        \"average\": avg_length,\n",
    "        \"min\": min_length,\n",
    "        \"max\": max_length,\n",
    "        \"median\": median_length,\n",
    "        \"std_dev\": std_dev\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42336bfa-0fc5-4afd-8ac1-484d91efe4a3",
   "metadata": {},
   "source": [
    "### Load BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5690759-4ae9-4772-8fe0-3bb5b67a4868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e373c1c1464da3aa643f7a062d16ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"GPT_mature_star_bpe_hairpin_tokenizer.json\") # GPT_mature_star_bpe_hairpin_tokenizer / \"gpt_rna_fine_tuned_ms/tokenizer.json\" \"GPT_mature_star_bpe_hairpin_tokenizer.json\"# loading the tokenizer from the pretrained training with the encoding of mature+star\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt_rna_mirgene_fine_tuned_no_flanks_ms\")  # no flanks!\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = {\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"bos_token\": \"<SOS>\",\n",
    "    \"eos_token\": \"<EOS>\",\n",
    "    \"unk_token\": \"<UNK>\"\n",
    "}\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "MAX_SEQ_LEN = max([len(s) for s in encoded_human_sequences]) \n",
    "\n",
    "def tokenize_function(data):\n",
    "    # Tokenize the sequences\n",
    "    output = tokenizer(data[\"sequences\"], truncation=True, padding='max_length', max_length=MAX_SEQ_LEN)\n",
    "    # Use input_ids as labels\n",
    "    output[\"labels\"] = output[\"input_ids\"].copy()\n",
    "    return output\n",
    "\n",
    "dataset = Dataset.from_dict({\"sequences\": encoded_human_sequences})\n",
    "\n",
    "tokenized_human_data = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6203452a-080a-45c4-907d-8d33ab2a1cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_human_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cae7c90-bd6e-4db2-89e5-12a518750b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf471a-ef3b-48de-aa56-ebc76d7de599",
   "metadata": {},
   "source": [
    "### Fine tune on Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f828bde7-7c70-4169-bae4-826a8ac16458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5280' max='5280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5280/5280 09:41, Epoch 80/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune on human, based on gff and mirgeneDB\n",
    "# Load the pre-trained model from Hugging Face\n",
    "model_name = \"mirgene_ms_all_data_hairpin\"  # mirgene_ms_hairpin\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define fine-tuning arguments with a smaller learning rate\n",
    "fine_tuning_args = TrainingArguments(\n",
    "    output_dir='./human_ms_all_data_hairpin_results',   # / human_ms_hairpin_all_data_results / human_ms_hairpin_results\n",
    "    num_train_epochs=80,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./human_ms_all_data_hairpin_logs',   # / human_ms_hairpin_logs\n",
    "    learning_rate=1e-5,  # smaller learning rate for fine-tuning\n",
    ")\n",
    "\n",
    "# Create a trainer instance for fine-tuning\n",
    "fine_tuning_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=fine_tuning_args,\n",
    "    train_dataset= tokenized_human_data,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tuning_trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"human_ms_all_data_hairpin_mirgendb_gff\")  #human_ms_hairpin_mirgendb_gff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cf935-6bb3-4fed-ad79-add859752f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2658920d-0c36-48d7-8c44-4b1f4a76732f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"GPT_mature_star_bpe_hairpin_tokenizer.json\") #GPT_mature_star_bpe_hairpin_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317c1ac-76bc-4ee2-8a57-d1d1db5c66a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate with prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21e6542a-2249-4db8-8a3a-d791f79e77f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 unique sequences without the last 388 characters.\n"
     ]
    }
   ],
   "source": [
    "# Set logging level to error to avoid the informational logs\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"human_ms_mirgendb_gff\") # human_ms_hairpin_mirgendb_gff / gpt_fine_tuned_human_mirgendb_gff\n",
    "\n",
    "# Ensure that the tokenizer's pad_token is set if it's not already\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "avg_length = calculate_statistics(original_human_sequences)['Average sequence length'] // 2\n",
    "\n",
    "generated_sequences = set()  # Use a set to store unique generated sequences\n",
    "unique_sequences = []  # List to maintain the order of generation\n",
    "prefixes = []\n",
    "\n",
    "test_data = pd.read_csv(\"/sise/vaksler-group/IsanaRNA/Transformers/GPT_env/seq_clusters/human_test_data.csv\")\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    if row['prime'] == '5p':\n",
    "        if relevant_seq == 'full_seq':\n",
    "            prefixes.append(row['full_seq'][:int(row['Mature_end'])+1])\n",
    "        else:\n",
    "            prefixes.append(row['Mature'])\n",
    "    else:\n",
    "        if relevant_seq == 'full_seq':\n",
    "            prefixes.append(row['full_seq'][:int(row['End_star'])+1])\n",
    "        else:\n",
    "            prefixes.append(row['Star'])\n",
    "\n",
    "for prefix in prefixes:\n",
    "    input_text = prefix\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    while True:\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_length=avg_length,\n",
    "            do_sample=True,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Decode without skipping special tokens to find and remove them\n",
    "        generated_sequence = tokenizer.decode(generated[0])\n",
    "        # Check for unwanted special tokens in the middle of the sequence\n",
    "        if \"<SOS>\" in generated_sequence[4:] or \"<EOS>\" in generated_sequence[:-4]:\n",
    "            break\n",
    "        cleaned_sequence = generated_sequence.replace(' ', '')\n",
    "\n",
    "        # Check if the generated sequence is unique\n",
    "        if cleaned_sequence not in generated_sequences:\n",
    "            generated_sequences.add(cleaned_sequence)\n",
    "            unique_sequences.append(cleaned_sequence)\n",
    "            break\n",
    "\n",
    "final_sequences = list(generated_sequences)\n",
    "\n",
    "print(f\"Generated {len(final_sequences)} unique sequences without the last {i} characters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c62553-be53-4775-8529-75c295d1e11c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate full seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d91e5e9-a1d0-44e9-adc5-44ba19a1983c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "100%|██████████| 100/100 [00:16<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 unique sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate without splitting\n",
    "\n",
    "# Set logging level to error to avoid the informational logs\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"human_ms_all_data_hairpin_mirgendb_gff\") # gpt_fine_tuned_human_mirgendb_gff\n",
    "\n",
    "# Ensure that the tokenizer's pad_token is set if it's not already\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "unique_sequences = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "avg_length = calculate_statistics(original_human_sequences)['Average sequence length'] // 2\n",
    "\n",
    "generated_sequences = set()  # Use a set to store unique generated sequences\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_text = \"<SOS>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    while True:\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_length=avg_length,\n",
    "            do_sample=True,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Decode without skipping special tokens to find and remove them\n",
    "        generated_sequence = tokenizer.decode(generated[0])\n",
    "\n",
    "        # Check for unwanted special tokens in the middle of the sequence\n",
    "        if \"<SOS>\" in generated_sequence[4:] or \"<EOS>\" in generated_sequence[:-4]:\n",
    "            break\n",
    "\n",
    "        cleaned_sequence = generated_sequence.replace(' ', '')\n",
    "\n",
    "        # Check if the generated sequence is unique\n",
    "        if cleaned_sequence not in generated_sequences:\n",
    "            generated_sequences.add(cleaned_sequence)\n",
    "            unique_sequences.append(cleaned_sequence)\n",
    "            break\n",
    "\n",
    "final_sequences = list(generated_sequences)\n",
    "\n",
    "print(f\"Generated {len(final_sequences)} unique sequences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e85c6522-1e09-487f-95e7-1347d9f5512a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of sequences': 100, 'Average sequence length': 202, 'Min sequence length': 155, 'Max sequence length': 238}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_statistics(generated_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f6672-a79c-4315-9126-60aabfe761d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generated to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9d500a7-3324-4762-a35f-9801654ed23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 699050.67it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/generated_100_full_data_hairpin_human.txt', 'w') as f: #generated_100_hairpin_full_data_human\n",
    "    for string in tqdm(generated_sequences):\n",
    "        string = string.replace('<SOS>','').replace('<EOS>','')\n",
    "        f.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d13c71f-a47c-4650-a866-11ee1190e46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_sequences = []\n",
    "with open('data/generated_100_full_data_hairpin_human.txt', 'r') as f: #generated_100_full_data_hairpin_human / generated_500_human_ms / genreated_new_10000_human_no_flanks_mirgenedb_gff_ms\n",
    "    for line in f:\n",
    "        # Add each line to the list after stripping newline characters\n",
    "        generated_sequences.append(line.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT_ron",
   "language": "python",
   "name": "gpt_ron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
